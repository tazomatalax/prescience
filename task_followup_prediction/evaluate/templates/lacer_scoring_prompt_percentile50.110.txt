You will be given a pair of title-abstracts Reference and Generated. Your task is to assign a score from 1-10 measuring how similar the Generated is to the Reference title-abstract where 1 represents the lowest similarity score and 10 represents the highest.

Here are some examples of References along with 10 Generations each corresponding to scores 1-10.

Reference A:
Intelligent Urban Surveillance in India: Real-Time Recognition of Individual Attributes
This study introduces a smart surveillance system for Indian cities that performs real-time identification and analysis of people's attributes. Leveraging advanced artificial intelligence and machine learning, the system recognizes features such as upper-body color, clothing, accessories, headgear, and so on, and analyzes behavior using camera feeds installed throughout the city.

Generations:
PyTorch: A High-Performance Deep Learning Library with an Imperative, Pythonic Design
Deep learning frameworks have often emphasized either ease of use or performance, but seldom both. PyTorch demonstrates that these aims can be jointly achieved: it provides an imperative, Python-native programming model that treats code as the model, simplifies debugging, and harmonizes with popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this work, we articulate the principles that guided PyTorch's implementation and show how they manifest in its architecture. We underscore that every part of PyTorch is an ordinary Python program under the user's full control. We also describe how a careful, pragmatic realization of the runtime's key components enables them to interoperate to deliver compelling performance. Finally, we present evidence of the efficiency of individual subsystems and the overall speed of PyTorch on several common benchmarks.
Score: 1

PyTorch-Stream: Eager, High-Throughput Extensions for Real-Time Vision and Video
We present PyTorch-Stream, a set of runtime and library extensions that bring low-latency, multi-stream video analytics to PyTorch while preserving its imperative programming model. PyTorch-Stream introduces asynchronous camera ingest, zero-copy decoding, fused preprocessing ops, and CUDA graph capture to minimize per-frame overhead. We describe design decisions that retain Pythonic control flow yet enable ahead-of-time export of hot paths for steady-state performance. Across common video tasks (object detection, tracking-by-detection, and attribute classification), PyTorch-Stream reduces end-to-end latency by 30-55% and increases throughput by up to 2.1× on a single GPU without sacrificing debuggability.
Score: 2

PyTorch-PersonAttr: A Reproducible Toolkit for Person Attribute Recognition in Surveillance Video
We release PyTorch-PersonAttr, a modular toolkit for training, evaluating, and deploying person attribute recognition models using the PyTorch-Stream runtime. The toolkit standardizes data adapters (e.g., RAP, PA-100k, PETA), implements strong CNN and hybrid CNN-Transformer baselines, and provides consistent evaluation protocols for multi-label classification (e.g., apparel type, color, accessories). We detail reference training recipes, mixed-precision/compiled execution for real-time inference, and uncertainty calibration methods for attribute outputs. Experiments show state-of-the-art or competitive results on public benchmarks and >60 FPS inference for 720p streams on a single GPU.
Score: 3

CityAttr: A Benchmark for Fine-Grained Person Attribute Recognition in Urban Camera Feeds
We introduce CityAttr, a benchmark targeting fine-grained person attributes in urban environments, with 250k annotated person crops spanning diverse weather, lighting, and camera viewpoints. CityAttr adds difficult classes pertinent to city operations (upper-body color under motion blur, reflective accessories, safety gear) and defines standardized train/val/test splits and occlusion strata. We provide strong PyTorch-PersonAttr baselines, error taxonomies, and fairness analyses across scene contexts. Results highlight failure modes in crowded scenes and under low light, motivating temporal modeling and domain adaptation for real-time deployments.
Score: 4

TorchStream City: A Low-Latency, Multi-Camera Video Analytics System for Urban Attribute Tagging
Building on PyTorch-Stream and CityAttr, we present TorchStream City, a reference system for city-scale, real-time attribute tagging. The system integrates multi-camera ingest, on-GPU batching/scheduling, online multi-object tracking, and temporal ensembling for stable attribute estimates. A hybrid eager-compiled execution path allows imperative control for orchestration while compiling per-operator graphs for steady-state throughput. Deployed on traffic and pedestrian cameras, TorchStream City sustains 25-30 FPS per stream across 32 feeds on two GPUs, with <200 ms median latency for detection, tracking, and attribute tagging, and provides APIs for downstream behavior analytics.
Score: 5

EdgeTorch-Attr: Efficient Person Attribute Recognition on Embedded and On-Camera Accelerators
We propose EdgeTorch-Attr, an optimization stack enabling person attribute recognition on edge devices. Techniques include compound scaling, quantization-aware training, structured pruning, knowledge distillation from server-grade models, and deployment via PyTorch 2 AOT compilation and TensorRT backends. We co-design the streaming pipeline to overlap decode, preproc, and inference on limited memory budgets. Field trials on NVIDIA Jetson- and VPU-class devices show 3-5× energy efficiency gains while maintaining >90% of server-level accuracy on CityAttr, with robust real-time performance on 1080p input for core attributes (upper-body color, apparel category, headgear, accessories).
Score: 6

IndAttr: Culturally-Aware Attribute Recognition for Indian Urban Scenes via Domain-Adaptive Temporal Transformers
We present IndAttr, a domain-adaptive model suite tailored to Indian urban video feeds. IndAttr augments attribute taxonomies to include culturally salient apparel and accessories (e.g., kurta, sari, dupatta, school uniforms, turbans, safety vests), and leverages temporal Vision Transformers with cross-frame attention for stability under occlusion and crowding. We introduce unsupervised domain adaptation from CityAttr using camera-style augmentation and self-training, plus limited expert-labeled Indian samples collected under ethical protocols. IndAttr improves mAP by 7.8 points over generic baselines on Indian pilot datasets while sustaining edge-ready throughput via distillation to compact temporal backbones.
Score: 7

PriCity-Attr: Privacy-Preserving, Federated Multi-Camera Attribute Analytics for Indian Smart Cities
We develop PriCity-Attr, a privacy-preserving training and deployment framework for multi-camera attribute analytics in Indian cities. The system performs on-device anonymization (face/body blurring for raw frames), stores only attribute and trajectory metadata, and trains models via cross-ward federated learning with secure aggregation and differential privacy accounting. We include governance hooks (policy-driven retention, audit logs) and human-in-the-loop tools for bias auditing. In a city pilot across 120 cameras, PriCity-Attr maintains IndAttr-level accuracy, reduces cross-site generalization error by 25%, and meets sub-250 ms end-to-end latency, while adhering to documented privacy constraints.
Score: 8

Real-Time Indian Urban Monitoring: City-Scale Deployment of Attribute and Behavior Analytics
We present a production-grade, city-scale deployment that integrates IndAttr and PriCity-Attr into a unified smart monitoring platform for Indian metros. The system performs real-time identification of salient person attributes (upper-body color, apparel type, accessories, headgear) and stabilizes outputs via temporal modeling to feed behavior analytics (zone occupancy, queueing, safety-gear compliance). An edge-cloud architecture supports heterogeneous cameras, with resilient scheduling, adaptive bitrate, and failover. We report month-long operations over 500+ feeds, covering throughput, latency, availability, and model drift monitoring, demonstrating actionable, privacy-aware analytics for urban management at scale.
Score: 9

Intelligent Urban Surveillance in India: Real-Time Recognition of Individual Attributes
This study introduces a smart surveillance system for Indian cities that performs real-time identification and analysis of people's attributes. Leveraging advanced artificial intelligence and machine learning, the system recognizes features such as upper-body color, clothing, accessories, headgear, and so on, and analyzes behavior using camera feeds installed throughout the city.
Score: 10

Reference B:
Ada-Instruct: Retooling Instruction Generation for Complex Reasoning
Augmenting instructions is critical for unlocking the full potential of large language models (LLMs) on downstream tasks. Current Self-Instruct approaches largely fabricate new instructions from a small seed set via in-context learning. Our analysis reveals a key limitation of this paradigm: even with GPT4o, Self-Instruct fails to produce complex instructions of length 100 or more, which are necessary for demanding tasks such as code completion. To overcome this, we observe that fine-tuning open source LLMs with only ten examples can yield complex instructions that preserve distributional consistency for complex reasoning tasks. We introduce Ada-Instruct, an adaptive instruction generator obtained through fine-tuning. We validate Ada-Instruct empirically across different applications, and the results demonstrate its ability to generate long, intricate, and distributionally consistent instructions.

Generations:
Evaluating Mathematical Problem Solving Using the MATH Dataset
Many forms of intellectual work depend on mathematical problem solving, yet computers still struggle with this capability. To assess this skill in machine learning systems, we present MATH, a dataset of 12,500 difficult competition mathematics problems. Each problem includes a complete step-by-step solution, enabling models to be trained to produce answer derivations and explanations. To support future research and improve accuracy on MATH, we additionally release a large auxiliary pretraining dataset aimed at teaching models the fundamentals of mathematics. While we are able to raise accuracy on MATH, our results show that performance remains relatively low, even with extremely large Transformer models. Moreover, if current scaling trends persist, simply expanding compute budgets and model parameter counts appears impractical for achieving strong mathematical reasoning. Although scaling Transformers is largely solving most other text-based tasks, it is not currently solving MATH. Substantive progress on mathematical problem solving will likely require new algorithmic advances from the broader research community.
Score: 1

Fine-Grained Evaluation of Mathematical Reasoning with Stepwise Derivations
We revisit evaluation on competition mathematics by exploiting the full step-by-step solutions accompanying problems. Beyond final-answer accuracy, we introduce derivation-aware metrics that align a model's generated reasoning with reference solutions via step matching, dependency agreement, and algebraic edit distance. We release standardized evaluation scripts and human-verified annotations for a subset of the MATH dataset, enabling assessment of intermediate reasoning quality and error localization. Our analysis across model scales shows that improvements in final accuracy often mask brittle derivations and shortcut patterns. We find that models benefiting from chain-of-thought prompting still diverge significantly from canonical derivations, suggesting the need for training signals that target the structure and granularity of mathematical reasoning rather than only end answers.
Score: 2

AutoHint-MATH: Augmenting Competition Problems with Model-Generated Hints and Subgoals
To bridge the gap between final-answer evaluation and robust derivations, we introduce AutoHint-MATH, a data augmentation pipeline that generates targeted hints, subgoals, and intermediate checks for competition-level math problems. Using large language models guided by our stepwise metrics, we produce pedagogically structured scaffolds that highlight crucial transformations and decision points. We curate 60k high-quality hint-problem-solution triplets and show that training with these augmentations yields consistent gains in both final answers and derivation alignment on MATH. Compared to vanilla chain-of-thought prompting, AutoHint-MATH improves sample efficiency and reduces off-track reasoning, indicating that explicit scaffolding can better shape model search through complex solution spaces.
Score: 3

InstructMATH: Instruction-Tuning for Step-by-Step Mathematical Problem Solving
We move from passive scaffolds to active task framing by introducing InstructMATH, a collection of instruction-problem-solution triples designed to instruction-tune models for mathematical reasoning. Instructions specify goals, permissible operations, and verification strategies, teaching models how to structure derivations before generating them. We compile 25k diverse instruction templates spanning algebra, geometry, and number theory, and show that instruction-tuned models outperform hint-augmented baselines on MATH under both few-shot and zero-shot settings. Analyses indicate that explicit procedural instructions reduce hallucinated steps and improve adherence to algebraic invariants. Our results suggest instruction-tuning is a promising path to controllable, reliable mathematical reasoning.
Score: 4

ReasonBench: A Multi-Domain Corpus of Instruction-Driven Problems with Worked Solutions
To test whether instruction-tuned reasoning transfers beyond mathematics, we create ReasonBench, a multi-domain corpus covering competition math, physics word problems, logic puzzles, and algorithmic thinking tasks, each paired with domain-specific instructions and worked solutions. We design a shared schema for instructions (goal, constraints, permitted tools, validation) and provide cross-domain transfer splits. Instruction-tuned models trained on ReasonBench improve on out-of-domain reasoning tasks, especially when instructions emphasize decomposition and verification. However, performance plateaus on tasks requiring long, intricate directives (e.g., multi-function code synthesis, multi-lemma proofs), motivating a closer study of instruction complexity and its role in reasoning.
Score: 5

On the Complexity of Instructions for Complex Reasoning Tasks
We systematically measure how instruction complexity-length, structural depth, and constraint density-affects performance on challenging tasks such as proof sketching and code completion. We find that instructions longer than ~100 tokens with nested constraints substantially improve reliability in multi-step reasoning, but current self-instruction pipelines rarely produce such long, structured directives and often drift from the target task's distribution. Using ReasonBench extensions for coding and formal reasoning, we quantify this mismatch and show that in-context self-instruct deteriorates instruction quality as length increases. These results highlight a bottleneck: generating long, distributionally faithful instructions is essential for unlocking complex reasoning, yet existing automatic pipelines fall short.
Score: 6

Scaffold-Instruct: Hierarchical Self-Instruct for Long-Form, Structured Prompts
We propose Scaffold-Instruct, a hierarchical self-instruction framework that builds long-form instructions via iterative decomposition: a planner drafts high-level goals, a refiner injects constraints and verification steps, and a validator enforces structural templates. Scaffold-Instruct increases instruction length and structural depth while improving internal consistency over standard self-instruct. On math, logic, and code tasks, models trained with Scaffold-Instruct show better adherence to multi-step procedures and fewer dead ends. Nevertheless, the approach is compute-intensive, relies on strong planners, and still exhibits distributional drift on specialized domains like code completion. These limitations motivate moving from purely in-context pipelines toward trainable instruction generators.
Score: 7

Few-Shot Fine-Tuned Instruction Generators for Mathematical and Program Reasoning
We introduce a trainable alternative to hierarchical self-instruct: fine-tuning a compact open-source LLM to act as an instruction generator using a small curated set (20-50) of long, structured seed instructions per domain. The resulting generator produces instructions that are longer, more constraint-aware, and more distributionally aligned with math and coding datasets than in-context self-instruct. When used to create training corpora, these instructions yield larger gains on multi-step reasoning and code completion benchmarks than Scaffold-Instruct, at a fraction of the cost. Ablations show that even minimal fine-tuning stabilizes long-instruction formatting and reduces semantic drift, suggesting few-shot fine-tuned generators are a scalable path forward.
Score: 8

AdaGen: Adaptive Few-Shot Instruction Generators for Complex Reasoning
Building on few-shot fine-tuned generators, we present AdaGen, an adaptive instruction generation framework that (1) fine-tunes open-source LLMs with as few as ten carefully selected seed instructions per domain, and (2) adaptively controls length, constraint density, and style to match target-task distributions. AdaGen automatically diagnoses gaps in coverage using lightweight discriminators and iteratively steers generation to maintain distributional fidelity. Across mathematical reasoning and code completion, AdaGen produces long (>=100 tokens), structurally rich instructions that lead to consistent downstream gains over self-instruct and hierarchical baselines. These findings set the stage for general-purpose, low-budget instruction generators tailored to complex reasoning tasks.
Score: 9

Ada-Instruct: Retooling Instruction Generation for Complex Reasoning
Augmenting instructions is critical for unlocking the full potential of large language models (LLMs) on downstream tasks. Current Self-Instruct approaches largely fabricate new instructions from a small seed set via in-context learning. Our analysis reveals a key limitation of this paradigm: even with GPT4o, Self-Instruct fails to produce complex instructions of length 100 or more, which are necessary for demanding tasks such as code completion. To overcome this, we observe that fine-tuning open source LLMs with only ten examples can yield complex instructions that preserve distributional consistency for complex reasoning tasks. We introduce Ada-Instruct, an adaptive instruction generator obtained through fine-tuning. We validate Ada-Instruct empirically across different applications, and the results demonstrate its ability to generate long, intricate, and distributionally consistent instructions.
Score: 10

Reference C:
Reinforced Self-Training (ReST) for Training Language Models
Reinforcement learning from human feedback (RLHF) can enhance the quality of large language models' (LLMs) outputs by aligning them with human preferences. We present a straightforward alignment approach, inspired by growing-batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Starting from an initial LLM policy, ReST constructs a dataset by sampling from that policy, and then uses offline RL algorithms on this data to further improve the policy. Because the training data are generated offline and can be reused, ReST is more efficient than standard online RLHF procedures. Although ReST is a general method applicable across generative learning scenarios, we center our investigation on machine translation. Our results demonstrate that ReST can substantially boost translation quality-according to both automated metrics and human evaluations on machine translation benchmarks-while being compute- and sample-efficient.

Generations:
Algorithms for Proximal Policy Optimization
We present a new class of policy gradient algorithms for reinforcement learning that alternate between collecting data through environment interaction and maximizing a surrogate objective via stochastic gradient ascent. Unlike standard policy gradient approaches that make only one gradient update per data sample, we introduce a novel objective that enables multiple epochs of minibatch updates. The resulting methods, termed proximal policy optimization (PPO), capture some of the benefits of trust region policy optimization (TRPO) while being much simpler to implement, more general, and empirically exhibiting better sample complexity. We evaluate PPO on a set of benchmark tasks, including simulated robotic locomotion and Atari game playing, and show that it outperforms other online policy gradient methods, achieving an overall favorable balance among sample complexity, simplicity, and wall-time.
Score: 1

Proximal Policy Optimization with Replay: Enabling Stable Data Reuse in On-Policy RL
We extend proximal policy optimization (PPO) with a principled form of experience replay that increases sample efficiency while preserving stability. Our method augments the clipped surrogate objective with lightweight off-policy corrections using per-batch importance weights and a target network for value learning, enabling multiple epochs over mixed recent on-policy and near-on-policy samples. Across MuJoCo locomotion and Atari benchmarks, PPO+Replay achieves comparable or better asymptotic performance than PPO with 30-50% fewer environment interactions, while retaining PPO's simplicity and wall-time advantages. Ablations isolate the effects of replay ratio and clipping on stability, providing a practical recipe for safe data reuse in on-policy policy gradient methods.
Score: 2

Proximal Sequence Policy Optimization for Text Generation
We adapt proximal policy optimization to autoregressive sequence models for text generation. Our approach optimizes a clipped sequence-level policy gradient objective over Transformer language models, with rewards defined by task-specific automated metrics (e.g., ROUGE for summarization, BLEU for translation). We introduce a token-wise advantage estimator that stabilizes long-horizon credit assignment and a curriculum over sequence lengths. Experiments on CNN/DailyMail summarization and WMT14 En-De translation show consistent metric gains over supervised fine-tuning and REINFORCE baselines at comparable compute, demonstrating that PPO's trust-region-like updates transfer effectively to discrete text generation.
Score: 3

KL-Regularized Proximal Optimization for Controlled Text Generation
We propose a KL-regularized variant of proximal optimization for sequence models that constrains updates relative to a supervised reference model. The objective augments PPO's clipped surrogate with an adaptive KL penalty against the reference policy, yielding stable improvements while preserving fluency. We instantiate rewards from automated metrics and task-specific constraints (length, toxicity filters), and introduce a reference-anchored value function to reduce variance. On summarization and translation, KL-Prox achieves higher ROUGE/BLEU and better human-rated coherence than metric-only PPO, highlighting the benefit of explicit distributional control for language generation.
Score: 4

Preference-Guided Proximal Optimization: Online RLHF for Sequence Models
We integrate human preference learning with proximal policy optimization for language generation. A reward model is trained from pairwise human comparisons and used to guide KL-regularized PPO updates of a Transformer policy anchored to a supervised reference. We interleave data collection (sampling candidate texts), preference labeling, reward model updates, and policy improvement. On summarization and open-ended generation, our method improves human preference win-rates over supervised baselines and metric-optimized PPO, while maintaining stability via adaptive KL and early stopping. Results demonstrate that online RLHF can be realized with simple proximal updates and modest human annotation.
Score: 5

Growing-Batch Preference Optimization: Bridging Online RLHF and Offline RL for LMs
We introduce a growing-batch training regime for preference-optimized language models that alternates between short bursts of sample collection and extended offline optimization. Candidate texts are generated from the current policy, scored by a learned preference model, and stored in a replay buffer. We then apply offline RL-style updates-advantage-weighted likelihood with conservative clipping and value targets fit by fitted policy evaluation-over the accumulated batch, with occasional refresh of the data. Compared to purely online PPO-based RLHF, Growing-Batch PO reduces the number of preference queries and environment interactions while achieving similar or better human preference win-rates on summarization and dialogue.
Score: 6

Self-Generated Corpora for Batch RLHF: Offline Advantage Optimization of LMs
We present a purely offline procedure for aligning language models using self-generated datasets. Starting from a supervised policy, we sample candidate continuations, score them with a learned preference model, and construct a static training set. We then perform multiple epochs of offline reinforcement learning using advantage-weighted regression with KL regularization to the reference, without further online sampling during optimization. This self-training loop can be repeated to grow the dataset. On summarization and translation, our batch RLHF approach matches or exceeds online PPO-based RLHF in human and automatic metrics while reducing compute and annotation overhead via data reuse.
Score: 7

Offline Reinforcement Learning for Machine Translation via Self-Generated Batches
Focusing on machine translation, we instantiate an offline RL framework that improves a Transformer translator using only self-generated candidate translations and offline optimization. We generate n-best lists from the current model, score them with a mixture of automatic metrics (BLEU, COMET) and a lightweight preference model, and train with KL-regularized advantage-weighted updates over the static pool. Periodic regeneration expands coverage, but all policy learning is performed offline, enabling aggressive data reuse. On WMT benchmarks, our method delivers substantial BLEU/COMET gains over supervised fine-tuning and online policy gradient methods at lower compute, with humans preferring our outputs for adequacy and fluency.
Score: 8

Preference-Scored Self-Training with Offline RL for LLM Translators
We propose a simple self-training algorithm for large language model (LLM) translators that combines dataset generation by the model with offline reinforcement learning. Given an initial LLM policy and a reference model, we produce candidate translations, score them using a learned preference model (optionally blended with automated metrics), and optimize the policy offline with KL-regularized advantage-weighted regression and value fitting. The process can be repeated in a growing-batch fashion to refresh the static dataset while retaining offline updates. Applied to WMT machine translation, this approach yields sizable improvements in BLEU/COMET and human evaluations relative to supervised fine-tuning and online RLHF, with favorable compute and sample efficiency.
Score: 9

Reinforced Self-Training (ReST) for Training Language Models
Reinforcement learning from human feedback (RLHF) can enhance the quality of large language models' (LLMs) outputs by aligning them with human preferences. We present a straightforward alignment approach, inspired by growing-batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Starting from an initial LLM policy, ReST constructs a dataset by sampling from that policy, and then uses offline RL algorithms on this data to further improve the policy. Because the training data are generated offline and can be reused, ReST is more efficient than standard online RLHF procedures. Although ReST is a general method applicable across generative learning scenarios, we center our investigation on machine translation. Our results demonstrate that ReST can substantially boost translation quality-according to both automated metrics and human evaluations on machine translation benchmarks-while being compute- and sample-efficient.
Score: 10

Reference D:
MAPO: Boosting Multilingual Reasoning by Casting Cross-Language Alignment as Preference Optimization
Although reasoning is often regarded as language-independent, current LLMs display uneven reasoning performance across languages; for instance, a dominant language like English typically outperforms others due to imbalanced multilingual training data. To strengthen reasoning in non-dominant languages, we introduce a Multilingual-Alignment-as-Preference Optimization framework (MAPO) that aligns the reasoning processes in other languages with those of the dominant language. In particular, we leverage an off-the-shelf translation model to enforce consistency between answers in non-dominant and dominant languages, and treat this consistency as a preference signal for optimization with methods such as Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Empirically, MAPO reliably delivers substantial improvements in multilingual reasoning across various models on three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and MNumGLUESub +13.3%), while also enhancing reasoning consistency across languages 1.

Generations:
Advancing Cross-Lingual Mathematical Reasoning: Empirical Findings and Lessons Learned
Prior work has largely emphasized building strong language models (LLMs) for mathematical reasoning in single-language settings, with limited attention to maintaining performance across multiple languages. To close this gap, we initiate the exploration and training of robust Multilingual Math Reasoning (xMR) LLMs. Using translation, we assemble the first multilingual instruction dataset for math reasoning, MGSM8KInstruct, spanning ten languages, thereby mitigating training data scarcity for xMR tasks. Leveraging this corpus, we introduce several training strategies to construct a family of xMR LLMs, dubbed MathOctopus, which consistently outperform typical open-source LLMs and surpass ChatGPT in few-shot setups. In particular, MathOctopus-13B attains 47.6% accuracy on the MGSM test set, exceeding ChatGPT's 46.3%. Beyond these results, our extensive experiments yield two key takeaways: (1) extending rejection sampling to multilingual settings provides performance gains, though the improvements are limited; and (2) using parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages significantly improves multilingual performance and also boosts monolingual ability. This suggests constructing multilingual corpora is a valuable lever for improving performance in a specific language, especially for mathematical reasoning. For example, MathOctopus-7B improves over English-only trained counterparts from 42.2% to 50.8% on the GSM8K test set.
Score: 1

Consistency-Regularized SFT for Multilingual Mathematical Reasoning
Building on multilingual instruction tuning for mathematical reasoning, we investigate whether explicitly encouraging cross-lingual agreement can further improve xMR LLMs. Using the MGSM8KInstruct corpus, we construct parallel problem-solution-rationale tuples across ten languages via high-quality translation. We introduce a consistency-regularized supervised fine-tuning (CR-SFT) objective that augments standard SFT with two auxiliary signals: (1) answer-level agreement across parallel languages, enforced via a differentiable dual-decoding consistency loss; and (2) rationale-level agreement, measured by semantic similarity between chain-of-thought embeddings. Fine-tuning 7B-13B models with CR-SFT yields consistent gains over MathOctopus, with the 7B model improving by +2.4% on MGSM and +1.8% on GSM8K. We observe that small amounts of multilingual parallel supervision are sufficient to regularize cross-lingual reasoning behavior, reducing variance across languages while preserving English performance.
Score: 2

Cross-Lingual Distillation for Multilingual Math Reasoning
We extend consistency-regularized multilingual SFT with a teacher-student framework that distills reasoning skills from a strong English teacher into a multilingual student. Given parallel MGSM8KInstruct data, the teacher generates high-quality rationales and answers in English, which are translated and used to supervise the student in non-dominant languages. We combine teacher-forced rationale distillation with consistency losses on final answers, and introduce a selective distillation filter to discard low-confidence teacher traces. On MGSM and a bilingual subset of MSVAMP, our 7B student outperforms CR-SFT by +1.5-2.1% across non-dominant languages while maintaining English accuracy, indicating that cross-lingual distillation transfers not only final-answer competence but also intermediate reasoning structures.
Score: 3

Round-Trip Self-Training for Low-Resource Multilingual Reasoning
To reduce reliance on parallel supervision, we propose round-trip self-training (RT-ST) that bootstraps multilingual mathematical reasoning from monolingual non-English data. RT-ST generates pseudo-labeled rationales by translating non-English problems to English, invoking the English teacher to produce rationales/answers, and translating them back. We introduce round-trip validation-requiring that translating the generated rationale back to English recovers an equivalent solution-and an error-aware filter using a symbolic solver. Training with a mix of distilled parallel data and RT-ST pseudo-labels improves low-resource languages by +2.7% average over cross-lingual distillation alone on MGSM, with negligible degradation in high-resource languages. This demonstrates that round-trip consistency can safely expand multilingual training corpora.
Score: 4

Step-Contrast: Contrastive Alignment of Cross-Lingual Reasoning Traces
We study whether aligning the structure of reasoning across languages yields further gains beyond answer consistency. Step-Contrast introduces a contrastive objective over token-level reasoning steps: steps that correspond across translations are pulled together in an embedding space, while steps from unrelated problems are pushed apart. We obtain soft step correspondences using attention-based alignment between bilingual rationales. Step-Contrast is combined with SFT and RT-ST to yield a unified objective that encourages both accurate and structurally aligned reasoning. On MGSM and MSVAMP, Step-Contrast reduces cross-language variance of step-level entailment by 15% and improves final-answer accuracy by +1.9% averaged over six non-dominant languages, suggesting that structural alignment complements consistency training.
Score: 5

XM-Reward: Cross-Lingual Consistency Reward Modeling for Reasoning
Moving beyond purely supervised objectives, we introduce XM-Reward, a learned reward model that scores multilingual reasoning traces by cross-lingual faithfulness and correctness. XM-Reward leverages (i) translation-consistency checks between non-dominant and English rationales/answers, (ii) symbolic verification when applicable, and (iii) fluency and step-validity heuristics. We train XM-Reward on preference pairs derived from Step-Contrast outputs and filtered by round-trip validation. Using offline policy optimization with reward-weighted SFT and KL-regularization, we improve over Step-Contrast by +1.6% on MGSM and +2.4% on MSVAMP while further reducing cross-lingual inconsistency. XM-Reward highlights the promise of learned evaluators that unify correctness and alignment signals for multilingual reasoning.
Score: 6

MPO: Multilingual Preference Optimization via Consistency-Derived Feedback
We convert cross-lingual consistency signals into pairwise preferences to directly optimize the policy with preference learning. Our Multilingual Preference Optimization (MPO) framework constructs, for each problem and language, pairs of model-generated traces with different cross-lingual agreement and solver-verified correctness. We study Direct Preference Optimization (DPO) vs. likelihood-ratio methods on this multilingual preference data, initialized from the XM-Reward-tuned model. MPO enhances both answer accuracy and cross-language agreement: on MGSM, it yields +3.8% over XM-Reward in non-dominant languages; on MSVAMP, +3.1%. Ablations show that anchoring preferences to English-consistent traces is more effective than intra-language preferences alone, motivating alignment to a dominant-language reference.
Score: 7

Anchor-Align PO: Dominant-Language Anchored Preference Optimization for xMR
We formalize dominant-language anchoring in preference optimization. Anchor-Align PO uses an off-the-shelf translation model to map non-dominant language traces to English, and defines preferences by (1) answer equivalence to a trusted English reference, (2) rationale consistency measured by bilingual entailment, and (3) solver-backed correctness. We train with both DPO and PPO variants, balancing cross-lingual alignment with exploration via KL control. Evaluated on MGSM, MSVAMP, and a subset of MNumGLUESub, Anchor-Align PO improves non-dominant language performance by +5.6% over MPO and reduces reasoning divergence from English by 20% relative. The method is data-agnostic, requiring no additional human labels beyond existing bilingual corpora and translation.
Score: 8

Towards MAPO: Multilingual-Alignment-as-Preference Optimization for Reasoning
We unify consistency-driven preference learning into a general Multilingual-Alignment-as-Preference Optimization (MAPO) paradigm. MAPO defines preferences by aligning non-dominant language reasoning to a dominant-language anchor using translation-mediated agreement on both answers and intermediate steps, supervised by solver checks when feasible. The framework supports DPO for stable offline alignment and PPO for online refinement, and plugs into any base LLM. Across three benchmarks-MSVAMP, MGSM, and MNumGLUESub-MAPO yields robust gains over Anchor-Align PO (+1.5-3.0% absolute) and consistently narrows cross-language performance gaps without harming English accuracy. Analyses show that aligning the process, not just the outcome, is key to durable multilingual reasoning improvements, setting the stage for scalable MAPO deployments.
Score: 9

MAPO: Boosting Multilingual Reasoning by Casting Cross-Language Alignment as Preference Optimization
Although reasoning is often regarded as language-independent, current LLMs display uneven reasoning performance across languages; for instance, a dominant language like English typically outperforms others due to imbalanced multilingual training data. To strengthen reasoning in non-dominant languages, we introduce a Multilingual-Alignment-as-Preference Optimization framework (MAPO) that aligns the reasoning processes in other languages with those of the dominant language. In particular, we leverage an off-the-shelf translation model to enforce consistency between answers in non-dominant and dominant languages, and treat this consistency as a preference signal for optimization with methods such as Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Empirically, MAPO reliably delivers substantial improvements in multilingual reasoning across various models on three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and MNumGLUESub +13.3%), while also enhancing reasoning consistency across languages 1.
Score: 10

Reference E:
ConDaFormer: A Decomposed Transformer with Local Structure Boosting for 3D Point Cloud Analysis
Recent studies have applied transformers to 3D point cloud understanding with notable success. However, the sheer number of points-often over 0.1 million-renders global self-attention impractical for point clouds. Consequently, many methods confine the transformer to local neighborhoods, such as spherical or cubic windows. Even then, the volume of Query-Key pairs remains large, incurring high computational overhead. Moreover, prior approaches typically derive queries, keys, and values via linear projections, neglecting explicit modeling of local 3D geometric structure. To tackle both efficiency and local geometry priors, we propose a new transformer block, ConDaFormer. Specifically, ConDaFormer decomposes a cubic window into three orthogonal 2D planes, thereby reducing the number of points involved while maintaining a comparable attention span. This disassembly enables a larger attention range without increasing computational complexity, albeit at the risk of missing some contextual information. To address this, we introduce a local structure enhancement strategy that applies a depth-wise convolution before and after the attention, which also reinforces the encoding of local geometric cues. With these components, ConDaFormer captures both long-range contextual dependencies and local priors. We demonstrate its effectiveness through experiments on multiple 3D point cloud understanding benchmarks. Code is available at https://github.com/LHDuan/ConDaFormer .

Generations:
Swin: A Hierarchical Vision Transformer Based on Shifted Window Attention
We introduce Swin Transformer, a vision Transformer that serves as a general-purpose backbone for computer vision. Adapting Transformers from language to vision is challenging due to domain differences, including the wide range of object scales in images and the far higher spatial resolution of pixels compared to words. To bridge this gap, we propose a hierarchical Transformer that computes representations using shifted windows. This shifted window strategy confines self-attention to non-overlapping local windows for efficiency, while still enabling connections across windows. The hierarchical design supports multi-scale modeling and achieves computational complexity that scales linearly with image size. These properties make Swin Transformer applicable to a broad set of tasks, from image classification (87.3 top-1 accuracy on ImageNet-1K) to dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). It surpasses prior state-of-the-art results by large margins of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, highlighting the potential of Transformer-based backbones for vision. The hierarchical formulation and shifted window mechanism also prove advantageous for all-MLP architectures. Code and models are available at https://github.com/microsoft/Swin-Transformer.
Score: 1

Swin-Stripe: Shifted Window Transformers with Orthogonal Strip Fusion for 2D Vision
We extend the shifted-window paradigm of hierarchical vision Transformers by introducing orthogonal strip attention that complements square local windows. In Swin-Stripe, self-attention is computed within non-overlapping windows as in Swin, while lightweight horizontal and vertical strip attentions bridge distant regions with negligible overhead. This hybrid local-global scheme increases the effective receptive field without abandoning linear complexity in image size. The architecture remains hierarchical and maintains cyclic shifts for cross-window connectivity. Experiments on image classification, object detection, and semantic segmentation show consistent gains over a pure windowed baseline, particularly on long, thin structures where strip fusion is beneficial.
Score: 2

Video Swin-Tube: Hierarchical Spatiotemporal Transformer with Shifted Tube Windows
We adapt shifted-window Transformers from images to videos by generalizing local windows into spatiotemporal tubes. Video Swin-Tube performs self-attention within space-time tubes and uses cyclic temporal-spatial shifts to exchange information across tube boundaries. To preserve efficiency, we augment tube attention with the orthogonal strip fusion from Swin-Stripe along spatial and temporal axes, enlarging context without incurring full global cost. The hierarchical design yields linear complexity in the number of frames and spatial resolution. On video action recognition and spatiotemporal segmentation benchmarks, Video Swin-Tube achieves competitive accuracy with significantly reduced memory compared to global attention baselines.
Score: 3

Voxel-Swin: Shifted Cubic Windows for 3D Volumetric Vision
We move from spatiotemporal grids to true 3D volumetric data, introducing Voxel-Swin, a hierarchical Transformer with shifted cubic windows. Analogous to its 2D and video counterparts, Voxel-Swin computes attention locally within cubes and uses cyclic shifts for cross-cube communication, enabling scalable modeling of large medical or scientific volumes. We analyze memory and compute trade-offs of cubic window sizes and demonstrate that Voxel-Swin attains strong performance on volumetric segmentation tasks (e.g., multi-organ CT, brain MRI), while maintaining near-linear complexity in voxel count. Results highlight the generality of shifted-window Transformers across 2D, 2.5D, and 3D grid-structured data.
Score: 4

GeoVoxel-Transformer: Geometry-Aware Local Attention on Sparse Voxelized Point Clouds
To address the inefficiency of dense volumetric grids for real-world 3D scenes, we propose GeoVoxel-Transformer, which operates on sparse voxelized point clouds. Building on shifted cubic windows, we introduce geometry-aware relative positional encodings and a lightweight depth-wise convolutional refinement applied after attention to inject local geometric priors. This design retains the hierarchical pyramid while respecting sparsity via submanifold operations. Evaluations on indoor scene segmentation and semantic scene completion demonstrate improved accuracy and efficiency over both dense volumetric Transformers and sparse 3D CNN baselines, highlighting the importance of geometry-aware attention on sparse data.
Score: 5

PlaneSwin: Disassembled Cubic Attention via Orthogonal Plane Aggregation
We further reduce the cost of local 3D attention by disassembling each cubic window into three orthogonal 2D planes (XY, YZ, ZX) and computing attention on these planes independently. PlaneSwin fuses the three plane-wise outputs to approximate cubic-context modeling while processing substantially fewer query-key pairs per pass. This disassembly enlarges the effective attention range at fixed complexity compared to purely cubic windows. Integrated into a sparse, hierarchical backbone, PlaneSwin yields faster inference and improved segmentation accuracy on large-scale indoor datasets. Ablations show that plane-wise attention provides a favorable accuracy-efficiency trade-off over cubic attention in sparse voxel settings.
Score: 6

PointPlane: Plane-wise Local Attention on Raw Point Clouds
We remove the voxelization step and introduce PointPlane, a Transformer operating directly on raw point clouds with plane-wise local attention. For each point, we form a local neighborhood via radius search and project neighbors onto three orthogonal 2D planes anchored to a canonical axis frame. Attention is computed per plane to capture long-range context within a controlled 2D manifold, then fused across planes and scales using a hierarchical pooling scheme. PointPlane preserves the efficiency of plane disassembly while avoiding voxel quantization artifacts. On classification and segmentation benchmarks, it outperforms voxel-based counterparts at comparable or lower computational cost.
Score: 7

PointPlane-LSE: Local Structure Enhanced Plane-wise Transformers for 3D Point Understanding
We enhance plane-wise point Transformers with an explicit local structure modeling module. PointPlane-LSE applies depth-wise convolutions on the projected plane maps both before attention (to encode geometric cues such as curvature and anisotropy) and after attention (to refine aggregated context). Coupled with a hierarchical subsampling and feature propagation strategy, this yields improved robustness to sampling density and noise. Experiments on indoor (S3DIS, ScanNet) and outdoor (SemanticKITTI) segmentation, as well as shape classification, show consistent gains over PointPlane and other local attention baselines, demonstrating the complementary roles of plane-wise attention and convolutional structure enhancement.
Score: 8

ConDaFormer-A: Disassembled Transformer with Adaptive Plane Decomposition for Point Clouds
We present ConDaFormer-A, a disassembled point cloud Transformer that generalizes orthogonal plane-wise attention to adaptive planes derived from local geometry. For each point neighborhood, we estimate a local frame via PCA and project neighbors onto three data-adaptive planes, enabling attention to align with surface orientation while retaining the computational benefits of 2D processing. As in PointPlane-LSE, depth-wise convolutions are applied before and after attention to encode and refine local structure. This adaptive decomposition improves modeling of anisotropic surfaces and thin structures, albeit with modest overhead for frame estimation. Across standard 3D benchmarks, ConDaFormer-A achieves strong accuracy-efficiency trade-offs and ablations highlight when adaptive versus fixed planes are preferable.
Score: 9

ConDaFormer: A Decomposed Transformer with Local Structure Boosting for 3D Point Cloud Analysis
Recent studies have applied transformers to 3D point cloud understanding with notable success. However, the sheer number of points-often over 0.1 million-renders global self-attention impractical for point clouds. Consequently, many methods confine the transformer to local neighborhoods, such as spherical or cubic windows. Even then, the volume of Query-Key pairs remains large, incurring high computational overhead. Moreover, prior approaches typically derive queries, keys, and values via linear projections, neglecting explicit modeling of local 3D geometric structure. To tackle both efficiency and local geometry priors, we propose a new transformer block, ConDaFormer. Specifically, ConDaFormer decomposes a cubic window into three orthogonal 2D planes, thereby reducing the number of points involved while maintaining a comparable attention span. This disassembly enables a larger attention range without increasing computational complexity, albeit at the risk of missing some contextual information. To address this, we introduce a local structure enhancement strategy that applies a depth-wise convolution before and after the attention, which also reinforces the encoding of local geometric cues. With these components, ConDaFormer captures both long-range contextual dependencies and local priors. We demonstrate its effectiveness through experiments on multiple 3D point cloud understanding benchmarks. Code is available at https://github.com/LHDuan/ConDaFormer .
Score: 10

Before providing the score, write about your reasoning about the similarity or dissimilarity. Immediately after that reasoning line, provide the similarity score.

Now please score the following pair of title-abstracts for similarity.

Reference:
{{reference_title}}
{{reference_abstract}}

Generation:
{{generation_title}}
{{generation_abstract}}

Use the following output format:
Reasoning: ...
Score: ...